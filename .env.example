# ============================================================================
# RAG API System - Environment Configuration Example
# ============================================================================
# Copy this file to .env and update with your actual values
# ============================================================================

# ============================================================================
# FLASK CONFIGURATION
# ============================================================================
# Flask environment mode
FLASK_ENV=development

# Flask server port
FLASK_PORT=5000

# Flask server host (0.0.0.0 for all interfaces)
FLASK_HOST=0.0.0.0

# Enable debug mode (set to False in production)
DEBUG=True

# ============================================================================
# REDIS CONFIGURATION
# ============================================================================
# Redis server hostname
REDIS_HOST=localhost

# Redis server port
REDIS_PORT=6379

# Redis database number (0-15)
REDIS_DB=0

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================
# SQLite database URL
# For PostgreSQL: postgresql://user:password@localhost/dbname
# For MySQL: mysql+pymysql://user:password@localhost/dbname
DATABASE_URL=sqlite:///./rag_system.db

# ============================================================================
# VECTOR STORE CONFIGURATION
# ============================================================================
# Path to store FAISS index files
FAISS_INDEX_PATH=./data/faiss_index

# Embedding model from Hugging Face
# Options:
#   - sentence-transformers/all-MiniLM-L6-v2 (384 dims, fast)
#   - sentence-transformers/all-mpnet-base-v2 (768 dims, better quality)
#   - sentence-transformers/paraphrase-MiniLM-L6-v2 (384 dims)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# ============================================================================
# LLM CONFIGURATION
# ============================================================================
# LLM model to use
# OpenAI: gpt-3.5-turbo, gpt-4
# Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229
# Groq: groq/llama-3.3-70b-versatile, groq/mixtral-8x7b-32768
# Cohere: command-r-plus
# Ollama (local): ollama/llama2
LLM_MODEL=groq/llama-3.3-70b-versatile

# API key for the LLM provider
# Get from: https://console.groq.com/keys (for Groq)
LLM_API_KEY=your-api-key-here

# LLM temperature (0.0 = deterministic, 1.0 = creative)
# Lower values = more focused/deterministic
# Higher values = more creative/random
LLM_TEMPERATURE=0.7

# ============================================================================
# TEXT PROCESSING CONFIGURATION
# ============================================================================
# Size of each text chunk in characters
# Smaller = more chunks, more precise retrieval
# Larger = fewer chunks, faster processing
CHUNK_SIZE=500

# Overlap between consecutive chunks in characters
# Helps maintain context between chunks
CHUNK_OVERLAP=50

# Maximum number of chunks to create per URL
# Prevents processing very large documents
MAX_CHUNKS_PER_URL=100

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Path to log file
LOG_FILE=./logs/app.log

# ============================================================================
# OPTIONAL: ADVANCED CONFIGURATION
# ============================================================================
# Worker timeout in seconds
WORKER_TIMEOUT=300

# Queue name for Redis
QUEUE_NAME=url_ingestion_queue

# ============================================================================
# NOTES
# ============================================================================
# 1. For production, set DEBUG=False and FLASK_ENV=production
# 2. Use strong API keys and store them securely
# 3. For PostgreSQL/MySQL, install: pip install psycopg2-binary or pymysql
# 4. For GPU support, install: pip install faiss-gpu
# 5. For local LLM, install Ollama: https://ollama.ai

